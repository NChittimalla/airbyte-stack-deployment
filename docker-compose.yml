version: '3.8'

services:
  # This repository contains the Docker Compose setup to quickly deploy the Modern Data Stack.

  # List of Services
  # 1.  postgres                  - 10.0.0.30
  # 3.  dagster                   - 10.0.0.28
  # 4.  dagster-daemon            - 10.0.0.27
  # 5.  mongodb                   - 10.0.0.26
  # 6.  opentelemetry-collector   - 10.0.0.25
  # 7.  nessie                    - 10.0.0.24
  # 8.  minio                     - 10.0.0.23
  # 9.  minio-setup               - 10.0.0.22
  # 10. dremio                    - 10.0.0.21
  # 12. spark                     - 10.0.0.19
  # 13. prometheus                - 10.0.0.18
  # 12. grafana                   - 10.0.0.17

  # Postgres is being leveraged for the dagster (dagster and dagster daemon) metadata and also for other testing purposes
  # 4 db's are created dagster, fnb, mydb, postgres (this is terces)
  postgres:
    image: postgres:13
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_USER: "dagster"
      POSTGRES_PASSWORD: "dagster"
      POSTGRES_DB: "dagster"
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_init.sql:/docker-entrypoint-initdb.d/postgres_init.sql
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dagster"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      terces:
        ipv4_address: 10.0.0.30

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter
    container_name: postgres_exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://dagster:dagster@postgres:5432/dagster?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    networks:
      terces:
        ipv4_address: 10.0.0.31

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml  # Mount the Prometheus configuration file
    networks:
      terces:
        ipv4_address: 10.0.0.32

  # opentelemetry-collector
  opentelemetry-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: opentelemetry-collector
    hostname: opentelemetry-collector
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "1888:1888" # pprof extension
      - "8889:8889" # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "4318:4318" # OTLP http receiver
      - "55679:55679" # zpages extension
    command: ["--config", "/etc/otel-collector-config.yml"]
    volumes:
      - ./opentelemetry-collector/data/otel-collector-config.yml:/etc/otel-collector-config.yml
    networks:
      terces:
        ipv4_address: 10.0.0.25

  metric-reporter:
    image: airbyte/metrics-reporter:${VERSION}
    container_name: metric-reporter
    environment:
      - DATABASE_PASSWORD=${DATABASE_PASSWORD}
      - DATABASE_URL=${DATABASE_URL}
      - DATABASE_USER=${DATABASE_USER}
      - METRIC_CLIENT=otel
      - OTEL_COLLECTOR_ENDPOINT=http://opentelemetry-collector:4317
    networks:
      terces:
        ipv4_address: 10.0.0.29  # Assigning a new IP for the metric reporter service

  # Minio Storage Server
  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=mthammu
      - MINIO_ROOT_PASSWORD=SecretDataStack5623#
      - MINIO_DOMAIN=storage
      - MINIO_REGION_NAME=us-east-1
      - MINIO_REGION=us-east-1
    container_name: minio
    hostname: minio-server
    ports:
      - 7001:9001
      - 7000:9000
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - minio_data:/data
    networks:
      terces:
        ipv4_address: 10.0.0.23

  # Minio Setup for creating buckets
  minio-setup:
    image: minio/minio:latest
    depends_on:
      - minio
    environment:
      - MINIO_ROOT_USER=mthammu
      - MINIO_ROOT_PASSWORD=SecretDataStack5623#
    volumes:
      - ./minio/sampledata:/tmp/data # Adjust this path to where your data files are stored
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for MinIO to start...' &&
      /bin/sleep 10 &&
      mc alias set minio http://minio:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD &&
      mc mb minio/lakehouse &&
      mc mb minio/landing &&
      mc mb minio/sampledata &&
      mc cp --recursive /tmp/data/* minio/sampledata/ &&
      echo 'Buckets created and data loaded.'"
    networks:
      terces:
        ipv4_address: 10.0.0.22

  dremio:
    image: dremio/dremio-oss:24.2
    ports:
      - 9047:9047
      - 31010:31010
      - 32010:32010
    container_name: dremio
    hostname: dremio
    environment:
      - DREMIO_JAVA_SERVER_HEAP_MAX=4G
      - DREMIO_JAVA_SERVER_HEAP_MIN=4G
      # Expose the metrics endpoint
      - DREMIO_WEB_ADMIN_HOST=0.0.0.0
      - DREMIO_WEB_ADMIN_PORT=9091
    volumes:
      - dremio_data:/opt/dremio/data
      - dremio_log:/opt/dremio/log
      - ./dremio.conf:/opt/dremio/conf/dremio.conf
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9047 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      terces:
        ipv4_address: 10.0.0.21

  # grafana
  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: '512M'
    networks:
      terces:
        ipv4_address: 10.0.0.17

volumes:
  postgres_data:
  minio_data:
  mongodb_data:
  dremio_data:
  dremio_log:
  spark_data:
    driver: local

networks:
  terces:
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.0.0/16
